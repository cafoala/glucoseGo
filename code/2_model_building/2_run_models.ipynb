{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "939504c9",
   "metadata": {},
   "source": [
    "# 3. Run ML Models\n",
    "This notebook executes both Logistic Regression and XGBoost models on the full set of features as well as a reduced set of top two features, aiming to compare the performance across these scenarios. The results are then visualized, specifically focusing on the ROC curves, and relevant performance metrics are stored for future reference and analysis.\n",
    "\n",
    "### Objective:\n",
    "To execute various machine learning models using the dataset provided and assess their performance on predicting the given outcome.\n",
    "\n",
    "### Data Overview:\n",
    "\n",
    "* Source: The data for this notebook is sourced from various CSV files located within directories defined in the notebook.\n",
    "* Features: The dataset contains a mix of numerical and categorical features. Some key features include 'start_glc', 'duration', and many others.\n",
    "* Target Variable: The prediction target is 'y_3', which is possibly a binary outcome indicating a certain event or condition.\n",
    "\n",
    "### Sections:\n",
    "\n",
    "1. Setup: Importing necessary libraries and defining paths.\n",
    "2. Data Loading: Reading the required datasets from their respective directories.\n",
    "3. Data Preparation: Setting up dataframes to store results and setting up predictor variables and target variable.\n",
    "4. Model Execution:\n",
    "        All Features:\n",
    "            Logistic Regression: Execution of logistic regression using all features, hyperparameter tuning, and storing of results.\n",
    "            XGBoost: Execution of XGBoost using all features and storing of results.\n",
    "        Top Two Features:\n",
    "            Logistic Regression: Execution of logistic regression using only the top two features, 'start_glc' and 'duration', and storing of results.\n",
    "            XGBoost: Execution of XGBoost using only the top two features and storing of results.\n",
    "5. Results Compilation: Storing of model results, calculation of mean results, and appending of results to dataframes.\n",
    "6. Data Saving: Storing results in specified directories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f34d55c8",
   "metadata": {},
   "source": [
    "## 3.0. Packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f38c273-ed18-4be0-8bfe-1bfae59294c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import ml_helper as ml_help\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import sys\n",
    "path = \"../../diametrics\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d2ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "directory = '../../data/tidy_data/final_df/'\n",
    "probability_results_directory = '../../results/probability_results/'\n",
    "k_fold_results_directory = '../../results/k_fold_results/'\n",
    "threshold_results_directory = '../../results/threshold_results/'\n",
    "mean_results_directory = '../../results/mean_results/'\n",
    "dict_directory = '../../results/dict_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84b2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(directory + 'df.csv')\n",
    "strat = df['stratify'] \n",
    "X = pd.read_csv(directory + 'X.csv')\n",
    "y = df['y'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b000108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09394914122716513"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f9fe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 414)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb450bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dfs for results\n",
    "df_with_probas = df.copy()\n",
    "mean_results = pd.DataFrame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e120f61a",
   "metadata": {},
   "source": [
    "## 3.1. All features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d157acc",
   "metadata": {},
   "source": [
    "### 3.1.1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LR model, uses hyperopt for HP tuning, get accuracy, indices and probabilities for each fold\n",
    "lr_all_k_fold_results, lr_all_test_sets_index, lr_all_predicted_probas, lr_all_observed, lr_all_shap_values, lr_all_coeffs, lr_all_hps = ml_help.k_fold_accuracies(X, y, strat, True, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in dictionary\n",
    "lr_all_results = {'X':X,\n",
    "              'probas':lr_all_predicted_probas, \n",
    "              'observed':lr_all_observed, \n",
    "              'shap':lr_all_shap_values, \n",
    "              'coeffs':lr_all_coeffs}\n",
    "\n",
    "with open(dict_directory+\"lr_results_all\", \"wb\") as fp:   \n",
    "    #Pickling \n",
    "    pickle.dump(lr_all_results, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mean accuracy to a table for easy perusal\n",
    "mean_results = ml_help.add_mean_to_df(mean_results, lr_all_k_fold_results, 'lr', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a probability column to the whole dataset to ensure \n",
    "df_with_probas = ml_help.add_proba_col(df_with_probas, lr_all_test_sets_index, lr_all_predicted_probas, 'probas_lr_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a17011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-fold results\n",
    "lr_all_k_fold_results.to_csv(k_fold_results_directory+'lr_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters\n",
    "pd.DataFrame(lr_all_hps).to_csv('../../results/hyperparameters/lr_all.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd647777",
   "metadata": {},
   "source": [
    "### 3.1.2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9647fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-10-25 15:31:35,438]\u001b[0m A new study created in memory with name: no-name-3e9b4530-1fe4-43e6-8228-81da8aae9d7f\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Run the XGBoost model, uses optuna for HP tuning, get accuracy, indices and probabilities for each fold\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_ts_k_fold_results, xgb_ts_test_sets_index, xgb_ts_predicted_probas, xgb_ts_observed, xgb_ts_shap, _, xgb_ts_hps \u001b[39m=\u001b[39m ml_help\u001b[39m.\u001b[39;49mk_fold_accuracies(X, y, strat, \u001b[39mFalse\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/projects/hypo-predict/code/2_model_building/ml_helper.py:297\u001b[0m, in \u001b[0;36mk_fold_accuracies\u001b[0;34m(X, y, strat_col, lr, features)\u001b[0m\n\u001b[1;32m    294\u001b[0m         pickle\u001b[39m.\u001b[39mdump(model, fp)\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     model \u001b[39m=\u001b[39m train_xgb(X_train, y_train)\n\u001b[1;32m    298\u001b[0m     model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    299\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../../results/models/xgb/\u001b[39m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mloop_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fp:   \n\u001b[1;32m    300\u001b[0m         \u001b[39m#Pickling \u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/hypo-predict/code/2_model_building/ml_helper.py:165\u001b[0m, in \u001b[0;36mtrain_xgb\u001b[0;34m(X_train, y_train, n_trials)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m# Optimize using Optuna\u001b[39;00m\n\u001b[1;32m    164\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: tune\u001b[39m.\u001b[39;49mxgb_objective(trial, X_train, y_train), n_trials\u001b[39m=\u001b[39;49mn_trials)\n\u001b[1;32m    166\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    168\u001b[0m \u001b[39m# Fit XGBoost model with the best parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m _optimize(\n\u001b[1;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    410\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/Desktop/projects/hypo-predict/code/2_model_building/ml_helper.py:165\u001b[0m, in \u001b[0;36mtrain_xgb.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m# Optimize using Optuna\u001b[39;00m\n\u001b[1;32m    164\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: tune\u001b[39m.\u001b[39;49mxgb_objective(trial, X_train, y_train), n_trials\u001b[39m=\u001b[39mn_trials)\n\u001b[1;32m    166\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    168\u001b[0m \u001b[39m# Fit XGBoost model with the best parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/hypo-predict/code/2_model_building/tune.py:120\u001b[0m, in \u001b[0;36mxgb_objective\u001b[0;34m(trial, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# Initiate a pruner to feed to the study\u001b[39;00m\n\u001b[1;32m    119\u001b[0m pruning_callback \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mintegration\u001b[39m.\u001b[39mXGBoostPruningCallback(trial, \u001b[39m\"\u001b[39m\u001b[39mtest-auc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m history \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mcv(param, dtrain, \n\u001b[1;32m    121\u001b[0m                     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m    122\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[pruning_callback], \n\u001b[1;32m    123\u001b[0m                     nfold\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m    124\u001b[0m                     stratified\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\u001b[39m#False?\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m                     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m    126\u001b[0m                     verbose_eval\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    127\u001b[0m                     seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m    129\u001b[0m \u001b[39m# Set n_estimators as a trial attribute; Accessible via study.trials_dataframe().\u001b[39;00m\n\u001b[1;32m    130\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(history))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/xgboost/training.py:496\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mbefore_iteration(booster, i, dtrain, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    495\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(i, obj)\n\u001b[1;32m    498\u001b[0m should_break \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mafter_iteration(booster, i, dtrain, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    499\u001b[0m res \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39maggregated_cv\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/xgboost/training.py:230\u001b[0m, in \u001b[0;36m_PackedBooster.update\u001b[0;34m(self, iteration, obj)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Iterate through folds for update'''\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcvfolds:\n\u001b[0;32m--> 230\u001b[0m     fold\u001b[39m.\u001b[39;49mupdate(iteration, obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/xgboost/training.py:216\u001b[0m, in \u001b[0;36mCVPack.update\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, iteration, fobj):\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbst\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtrain, iteration, fobj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_cgm_env_test/lib/python3.9/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1681\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1682\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1683\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the XGBoost model, uses optuna for HP tuning, get accuracy, indices and probabilities for each fold\n",
    "xgb_ts_k_fold_results, xgb_ts_test_sets_index, xgb_ts_predicted_probas, xgb_ts_observed, xgb_ts_shap, _, xgb_ts_hps = ml_help.k_fold_accuracies(X, y, strat, False, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in dictionary\n",
    "xgb_ts_results = {'X':X,\n",
    "                'probas':xgb_ts_predicted_probas, \n",
    "              'observed':xgb_ts_observed, \n",
    "              'shap':xgb_ts_shap\n",
    "              }\n",
    "\n",
    "with open(dict_directory+\"xgb_ts\", \"wb\") as fp:   \n",
    "    #Pickling \n",
    "    pickle.dump(xgb_ts_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c42660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mean accuracy to a table for easy perusal\n",
    "mean_results = ml_help.add_mean_to_df(mean_results, xgb_ts_k_fold_results, 'xgb', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ff94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a probability column to the whole dataset to ensure \n",
    "df_with_probas = ml_help.add_proba_col(df_with_probas, xgb_ts_test_sets_index, xgb_ts_predicted_probas , 'probas_xgb_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-fold results\n",
    "xgb_ts_k_fold_results.to_csv(k_fold_results_directory+'xgb_ts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters\n",
    "pd.DataFrame(xgb_ts_hps).to_csv('../../results/hyperparameters/xgb_ts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed931c5d-5670-414c-9bdf-803574a098fe",
   "metadata": {},
   "source": [
    "## 3.2. Two features\n",
    "Two features shown in the feature selection process to be the most important, start glucose and duration of exercise bout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc413e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the two features from feature selection\n",
    "X_two = X[['start_glc','duration']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09642357",
   "metadata": {},
   "source": [
    "### 3.2.1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475b054-8ca6-4beb-9d54-540d56bde7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:03<00:00, 19.39trial/s, best loss: -0.828102589572673] \n",
      "100%|██████████| 60/60 [00:01<00:00, 39.14trial/s, best loss: -0.8214115328602904]\n",
      "100%|██████████| 60/60 [00:01<00:00, 32.46trial/s, best loss: -0.8245795345759641]\n",
      "100%|██████████| 60/60 [00:01<00:00, 31.50trial/s, best loss: -0.8253699257912561]\n",
      "100%|██████████| 60/60 [00:01<00:00, 35.68trial/s, best loss: -0.8236815065259309]\n",
      "100%|██████████| 60/60 [00:02<00:00, 27.62trial/s, best loss: -0.8257703790871129]\n",
      "100%|██████████| 60/60 [00:02<00:00, 27.51trial/s, best loss: -0.8275854856569197]\n",
      "100%|██████████| 60/60 [00:02<00:00, 28.64trial/s, best loss: -0.8282459525585395]\n",
      "100%|██████████| 60/60 [00:02<00:00, 29.69trial/s, best loss: -0.8226750204137498]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.88trial/s, best loss: -0.8263983203357084]\n"
     ]
    }
   ],
   "source": [
    "# Run the LR model, uses hyperopt for HP tuning, get accuracy, indices and probabilities for each fold\n",
    "lr_two_k_fold_results, lr_two_test_sets_index, lr_two_predicted_probas, lr_two_observed, lr_two_shap_values, lr_two_coeffs, lr_two_hps = ml_help.k_fold_accuracies(X_two, y, strat, True, 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc62b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in dictionary\n",
    "lr_two_results = {'X':X_two,\n",
    "              'probas':lr_two_predicted_probas, \n",
    "              'observed':lr_two_observed, \n",
    "              'shap':lr_two_shap_values, \n",
    "              'coeffs':lr_two_coeffs}\n",
    "\n",
    "with open(dict_directory+\"lr_two\", \"wb\") as fp:   \n",
    "    #Pickling \n",
    "    pickle.dump(lr_two_results, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d00e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mean accuracy to a table for easy perusal\n",
    "mean_results = ml_help.add_mean_to_df(mean_results, lr_two_k_fold_results, 'lr', 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1fe908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a probability column to the whole dataset to ensure \n",
    "df_with_probas = ml_help.add_proba_col(df_with_probas, lr_two_test_sets_index, lr_two_predicted_probas, 'probas_lr_two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f490ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-fold results\n",
    "lr_two_k_fold_results.to_csv(k_fold_results_directory+'lr_two.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b8fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters\n",
    "pd.DataFrame(lr_two_hps).to_csv('../../results/hyperparameters/lr_two.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f429303a",
   "metadata": {},
   "source": [
    "### 3.2.2. XGB Two feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ac2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-10-26 13:49:51,935]\u001b[0m A new study created in memory with name: no-name-8d3db598-f09d-4ecc-a4fb-8d6abec35267\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:49:59,753]\u001b[0m Trial 0 finished with value: 0.8442211000000001 and parameters: {'n_estimators': 435, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.5861685334492945, 'colsample_bytree': 0.5813641858449137, 'eta': 0.14828502813262978, 'learning_rate': 0.36330793003393125, 'reg_alpha': 5, 'reg_lambda': 2, 'gamma': 1}. Best is trial 0 with value: 0.8442211000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:05,773]\u001b[0m Trial 1 finished with value: 0.8406878000000001 and parameters: {'n_estimators': 40, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.6602619082639372, 'colsample_bytree': 0.9243931970038819, 'eta': 0.2942641546509594, 'learning_rate': 0.020584057548178984, 'reg_alpha': 3, 'reg_lambda': 2, 'gamma': 5}. Best is trial 0 with value: 0.8442211000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:11,421]\u001b[0m Trial 2 finished with value: 0.8442734 and parameters: {'n_estimators': 564, 'max_depth': 8, 'min_child_weight': 9, 'subsample': 0.5369046764098062, 'colsample_bytree': 0.5641308301828827, 'eta': 0.19176696086970202, 'learning_rate': 0.49097080946679694, 'reg_alpha': 4, 'reg_lambda': 3, 'gamma': 2}. Best is trial 2 with value: 0.8442734.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:15,966]\u001b[0m Trial 3 finished with value: 0.8404098 and parameters: {'n_estimators': 467, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.5401433871413165, 'colsample_bytree': 0.5630387877812517, 'eta': 0.05751308388422986, 'learning_rate': 0.2366384690045706, 'reg_alpha': 1, 'reg_lambda': 0, 'gamma': 2}. Best is trial 2 with value: 0.8442734.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:20,494]\u001b[0m Trial 4 finished with value: 0.8403285 and parameters: {'n_estimators': 348, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.2632699968134814, 'colsample_bytree': 0.4923964494468761, 'eta': 0.1346644759050532, 'learning_rate': 0.3809279886883188, 'reg_alpha': 2, 'reg_lambda': 4, 'gamma': 0}. Best is trial 2 with value: 0.8442734.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:20,597]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:21,525]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:21,713]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:21,785]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:22,009]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:22,222]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:22,393]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:26,623]\u001b[0m Trial 12 finished with value: 0.8462586 and parameters: {'n_estimators': 587, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.7713957343641953, 'colsample_bytree': 0.3431203822559875, 'eta': 0.191884436059782, 'learning_rate': 0.4861057482125396, 'reg_alpha': 4, 'reg_lambda': 2, 'gamma': 3}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:26,755]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:26,916]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,100]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,255]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,394]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,580]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,687]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,802]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:27,914]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,121]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,252]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,380]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,612]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,737]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,837]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:28,968]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:29,100]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:32,892]\u001b[0m Trial 30 finished with value: 0.8390978 and parameters: {'n_estimators': 682, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6950191023816319, 'colsample_bytree': 0.609829874373934, 'eta': 0.12815269432737872, 'learning_rate': 0.49522083521120563, 'reg_alpha': 4, 'reg_lambda': 3, 'gamma': 0}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,020]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,125]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,261]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,357]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,487]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,724]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,852]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:33,966]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:34,063]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:34,185]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:34,596]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:39,314]\u001b[0m Trial 42 finished with value: 0.8451329 and parameters: {'n_estimators': 558, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.8746770706493221, 'colsample_bytree': 0.5516316863466609, 'eta': 0.1031742414329875, 'learning_rate': 0.22404498420412516, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 3}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:44,630]\u001b[0m Trial 43 finished with value: 0.8446519 and parameters: {'n_estimators': 571, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.9342652727948583, 'colsample_bytree': 0.4789676391296517, 'eta': 0.09877137603892046, 'learning_rate': 0.2478829418424779, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 3}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:44,812]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:45,003]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:45,756]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:46,545]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:51,408]\u001b[0m Trial 48 finished with value: 0.844671 and parameters: {'n_estimators': 734, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.8504799714408576, 'colsample_bytree': 0.6862214332505068, 'eta': 0.1043804485794271, 'learning_rate': 0.4022408027359672, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 4}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:56,202]\u001b[0m Trial 49 finished with value: 0.8443413999999999 and parameters: {'n_estimators': 763, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.8530686250609795, 'colsample_bytree': 0.6827959169497491, 'eta': 0.09909106320430892, 'learning_rate': 0.3907417260548446, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 4}. Best is trial 12 with value: 0.8462586.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:50:59,982]\u001b[0m A new study created in memory with name: no-name-33c88115-b447-4c69-9ccf-6ef944c4f956\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:03,018]\u001b[0m Trial 0 finished with value: 0.8385657 and parameters: {'n_estimators': 838, 'max_depth': 11, 'min_child_weight': 9, 'subsample': 0.4802453146651702, 'colsample_bytree': 0.7283741458092976, 'eta': 0.19761571352689739, 'learning_rate': 0.007569770923354248, 'reg_alpha': 5, 'reg_lambda': 2, 'gamma': 0}. Best is trial 0 with value: 0.8385657.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:07,433]\u001b[0m Trial 1 finished with value: 0.8510263 and parameters: {'n_estimators': 276, 'max_depth': 12, 'min_child_weight': 10, 'subsample': 0.8434421138130321, 'colsample_bytree': 0.7513174111467187, 'eta': 0.14978499505983206, 'learning_rate': 0.08186712810636962, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 5}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:11,268]\u001b[0m Trial 2 finished with value: 0.8474166000000001 and parameters: {'n_estimators': 220, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.48392694969499783, 'colsample_bytree': 0.9907886743931338, 'eta': 0.07053460961206394, 'learning_rate': 0.07106722687467454, 'reg_alpha': 0, 'reg_lambda': 3, 'gamma': 0}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:15,870]\u001b[0m Trial 3 finished with value: 0.8485005999999998 and parameters: {'n_estimators': 300, 'max_depth': 12, 'min_child_weight': 6, 'subsample': 0.9217960798469902, 'colsample_bytree': 0.9301991715460332, 'eta': 0.09477581342988832, 'learning_rate': 0.030967288370006996, 'reg_alpha': 5, 'reg_lambda': 5, 'gamma': 3}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:19,632]\u001b[0m Trial 4 finished with value: 0.8452085 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.642646792473168, 'colsample_bytree': 0.8007989110735465, 'eta': 0.23623275302044205, 'learning_rate': 0.009504852957690919, 'reg_alpha': 0, 'reg_lambda': 4, 'gamma': 1}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:19,732]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:19,894]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:23,506]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 55.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:23,623]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:23,702]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:23,805]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:23,903]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:24,002]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:24,100]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:28,261]\u001b[0m Trial 14 finished with value: 0.8498411000000001 and parameters: {'n_estimators': 635, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.7106833274674368, 'colsample_bytree': 0.6590173604716565, 'eta': 0.11398017101771024, 'learning_rate': 0.07890085503916788, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:32,165]\u001b[0m Trial 15 finished with value: 0.8492070999999999 and parameters: {'n_estimators': 644, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.6865875871478168, 'colsample_bytree': 0.6591366166887036, 'eta': 0.1753569064254196, 'learning_rate': 0.11133146090307765, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:35,880]\u001b[0m Trial 16 finished with value: 0.8481602 and parameters: {'n_estimators': 634, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.7375450019115247, 'colsample_bytree': 0.516017152852337, 'eta': 0.1139477929970463, 'learning_rate': 0.1524319013361079, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:35,999]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:36,826]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 20.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:36,932]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:37,034]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:40,532]\u001b[0m Trial 21 finished with value: 0.8489749999999999 and parameters: {'n_estimators': 630, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.696069506404872, 'colsample_bytree': 0.6596970270923859, 'eta': 0.18829758031100735, 'learning_rate': 0.10550942322500591, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:40,764]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:40,862]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:40,961]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:41,060]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:41,157]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:41,276]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:45,018]\u001b[0m Trial 28 finished with value: 0.8467022 and parameters: {'n_estimators': 542, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7734116633851497, 'colsample_bytree': 0.39542665042647007, 'eta': 0.15866725318558275, 'learning_rate': 0.22086293369282267, 'reg_alpha': 2, 'reg_lambda': 1, 'gamma': 1}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:45,151]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:45,279]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:48,786]\u001b[0m Trial 31 finished with value: 0.8494689 and parameters: {'n_estimators': 674, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.6868289029543886, 'colsample_bytree': 0.6430553033953719, 'eta': 0.18221525990756762, 'learning_rate': 0.10881754276474415, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:48,883]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:49,035]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:49,280]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:49,396]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:49,496]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:49,947]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:50,081]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:50,187]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:50,304]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:50,435]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:50,621]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:54,402]\u001b[0m Trial 43 finished with value: 0.8478394999999999 and parameters: {'n_estimators': 787, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.6897217050102641, 'colsample_bytree': 0.5619595394799776, 'eta': 0.2066403971090653, 'learning_rate': 0.13135074885204548, 'reg_alpha': 1, 'reg_lambda': 0, 'gamma': 2}. Best is trial 1 with value: 0.8510263.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:54,506]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:54,780]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:55,100]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:55,210]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:55,312]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:55,413]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:51:58,639]\u001b[0m A new study created in memory with name: no-name-837c7a3b-ebab-4bcb-aa45-d3494fc30d85\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:02,964]\u001b[0m Trial 0 finished with value: 0.8390005 and parameters: {'n_estimators': 950, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.36949756371293274, 'colsample_bytree': 0.20082105199862035, 'eta': 0.126964689107728, 'learning_rate': 0.3272062144102276, 'reg_alpha': 2, 'reg_lambda': 0, 'gamma': 2}. Best is trial 0 with value: 0.8390005.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:06,235]\u001b[0m Trial 1 finished with value: 0.8381103999999999 and parameters: {'n_estimators': 862, 'max_depth': 11, 'min_child_weight': 3, 'subsample': 0.5317810382281372, 'colsample_bytree': 0.8388381982732576, 'eta': 0.15166959359459498, 'learning_rate': 0.011936874856266436, 'reg_alpha': 3, 'reg_lambda': 2, 'gamma': 5}. Best is trial 0 with value: 0.8390005.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:08,865]\u001b[0m Trial 2 finished with value: 0.8477624 and parameters: {'n_estimators': 770, 'max_depth': 2, 'min_child_weight': 10, 'subsample': 0.27470381600886234, 'colsample_bytree': 0.6910151272046319, 'eta': 0.29869804019644564, 'learning_rate': 0.11316422599217309, 'reg_alpha': 2, 'reg_lambda': 5, 'gamma': 4}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:13,840]\u001b[0m Trial 3 finished with value: 0.8471162999999999 and parameters: {'n_estimators': 447, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.8483273731343812, 'colsample_bytree': 0.8125640009707098, 'eta': 0.12005831865427895, 'learning_rate': 0.3609408215823289, 'reg_alpha': 1, 'reg_lambda': 5, 'gamma': 3}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:18,553]\u001b[0m Trial 4 finished with value: 0.8465315999999999 and parameters: {'n_estimators': 826, 'max_depth': 9, 'min_child_weight': 6, 'subsample': 0.861266672740743, 'colsample_bytree': 0.8309909319309086, 'eta': 0.18698820588996318, 'learning_rate': 0.09149792393105785, 'reg_alpha': 0, 'reg_lambda': 1, 'gamma': 3}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:18,685]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:18,788]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:18,964]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,071]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,152]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,265]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,369]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,470]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,619]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,730]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:19,842]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:20,016]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:20,106]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:24,269]\u001b[0m Trial 18 finished with value: 0.8475024 and parameters: {'n_estimators': 210, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9126756482606764, 'colsample_bytree': 0.4538482636983851, 'eta': 0.09052103242929675, 'learning_rate': 0.4898272299585543, 'reg_alpha': 2, 'reg_lambda': 5, 'gamma': 3}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:24,378]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:24,581]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:24,750]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:24,879]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,203]\u001b[0m Trial 23 finished with value: 0.8476175000000001 and parameters: {'n_estimators': 590, 'max_depth': 12, 'min_child_weight': 9, 'subsample': 0.9182532720273011, 'colsample_bytree': 0.6326496590038735, 'eta': 0.1325949647185371, 'learning_rate': 0.40100140087470537, 'reg_alpha': 3, 'reg_lambda': 4, 'gamma': 2}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,339]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,467]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,567]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,664]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,764]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:30,941]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:31,085]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:31,285]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:31,398]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:31,564]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,281]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 54.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,383]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,506]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,703]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,809]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:34,905]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:35,019]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:35,313]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:35,472]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:35,571]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:36,237]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:36,349]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:36,480]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:36,961]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:37,068]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:40,321]\u001b[0m Trial 49 finished with value: 0.8471585000000001 and parameters: {'n_estimators': 842, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.8620818902969735, 'colsample_bytree': 0.4891165035192005, 'eta': 0.12258767139330032, 'learning_rate': 0.43006827755435034, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 5}. Best is trial 2 with value: 0.8477624.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:43,570]\u001b[0m A new study created in memory with name: no-name-18460554-2156-4e23-848f-8fdef4a745d2\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:48,005]\u001b[0m Trial 0 finished with value: 0.8483845999999999 and parameters: {'n_estimators': 615, 'max_depth': 11, 'min_child_weight': 5, 'subsample': 0.6713452097013901, 'colsample_bytree': 0.2698343562733576, 'eta': 0.14298721085823068, 'learning_rate': 0.4148732717387005, 'reg_alpha': 4, 'reg_lambda': 0, 'gamma': 3}. Best is trial 0 with value: 0.8483845999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:52,308]\u001b[0m Trial 1 finished with value: 0.8450835 and parameters: {'n_estimators': 580, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.6636894760033512, 'colsample_bytree': 0.5587221578580004, 'eta': 0.06460034230754551, 'learning_rate': 0.02783102570298209, 'reg_alpha': 0, 'reg_lambda': 2, 'gamma': 0}. Best is trial 0 with value: 0.8483845999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:55,179]\u001b[0m Trial 2 finished with value: 0.8392108 and parameters: {'n_estimators': 570, 'max_depth': 2, 'min_child_weight': 1, 'subsample': 0.5933661437043338, 'colsample_bytree': 0.5314142892016959, 'eta': 0.05556606337575344, 'learning_rate': 0.019052502603177198, 'reg_alpha': 4, 'reg_lambda': 2, 'gamma': 5}. Best is trial 0 with value: 0.8483845999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:52:59,029]\u001b[0m Trial 3 finished with value: 0.8499316 and parameters: {'n_estimators': 872, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.7464550441135671, 'colsample_bytree': 0.8585017093737999, 'eta': 0.06375259209681065, 'learning_rate': 0.08581364526576775, 'reg_alpha': 0, 'reg_lambda': 5, 'gamma': 4}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:02,459]\u001b[0m Trial 4 finished with value: 0.8484183999999999 and parameters: {'n_estimators': 261, 'max_depth': 4, 'min_child_weight': 6, 'subsample': 0.5667458257678063, 'colsample_bytree': 0.5846362365232249, 'eta': 0.16700212774641304, 'learning_rate': 0.05140437130929437, 'reg_alpha': 5, 'reg_lambda': 0, 'gamma': 2}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:02,533]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:02,630]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:02,698]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,273]\u001b[0m Trial 8 finished with value: 0.8465429 and parameters: {'n_estimators': 338, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.28105905040733137, 'colsample_bytree': 0.5909789109110715, 'eta': 0.18215072415840539, 'learning_rate': 0.1474613399163589, 'reg_alpha': 0, 'reg_lambda': 0, 'gamma': 3}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,367]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,569]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,665]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,766]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:07,890]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,081]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,182]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,276]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,420]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,563]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,765]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,892]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:08,998]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:13,320]\u001b[0m Trial 22 finished with value: 0.8471004000000001 and parameters: {'n_estimators': 848, 'max_depth': 11, 'min_child_weight': 4, 'subsample': 0.6722818911893965, 'colsample_bytree': 0.3398570335330352, 'eta': 0.1671351624532193, 'learning_rate': 0.4861031713819545, 'reg_alpha': 4, 'reg_lambda': 0, 'gamma': 2}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:13,473]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:13,684]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:13,811]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:13,957]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,087]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,221]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,367]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,634]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,820]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:14,951]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:15,211]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:19,555]\u001b[0m Trial 34 finished with value: 0.8485726 and parameters: {'n_estimators': 958, 'max_depth': 12, 'min_child_weight': 5, 'subsample': 0.7013091127119279, 'colsample_bytree': 0.5185466370862981, 'eta': 0.21001016122718166, 'learning_rate': 0.49589189349895213, 'reg_alpha': 5, 'reg_lambda': 1, 'gamma': 3}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:19,734]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:19,874]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:19,970]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:20,082]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:20,227]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:20,348]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:24,845]\u001b[0m Trial 41 finished with value: 0.8477967 and parameters: {'n_estimators': 863, 'max_depth': 11, 'min_child_weight': 5, 'subsample': 0.6869145763113521, 'colsample_bytree': 0.30063089563703144, 'eta': 0.1672191877616028, 'learning_rate': 0.4871756034288467, 'reg_alpha': 4, 'reg_lambda': 0, 'gamma': 2}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:29,308]\u001b[0m Trial 42 finished with value: 0.8488959000000001 and parameters: {'n_estimators': 969, 'max_depth': 11, 'min_child_weight': 5, 'subsample': 0.7576569880905148, 'colsample_bytree': 0.2675626194874429, 'eta': 0.2157583541073436, 'learning_rate': 0.3789294423072151, 'reg_alpha': 4, 'reg_lambda': 0, 'gamma': 2}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:29,556]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,391]\u001b[0m Trial 44 finished with value: 0.8493216 and parameters: {'n_estimators': 948, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.7536999104248898, 'colsample_bytree': 0.24400334048474062, 'eta': 0.23720008467785836, 'learning_rate': 0.3592749092665992, 'reg_alpha': 4, 'reg_lambda': 1, 'gamma': 2}. Best is trial 3 with value: 0.8499316.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,496]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,601]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,728]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,836]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:33,950]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:41,184]\u001b[0m A new study created in memory with name: no-name-6faccb4c-684d-4490-94e8-373080e4dfab\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:44,223]\u001b[0m Trial 0 finished with value: 0.8428305 and parameters: {'n_estimators': 576, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.22433324522748316, 'colsample_bytree': 0.28896018436668325, 'eta': 0.26686227803197576, 'learning_rate': 0.0063855446044881435, 'reg_alpha': 1, 'reg_lambda': 0, 'gamma': 2}. Best is trial 0 with value: 0.8428305.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:48,783]\u001b[0m Trial 1 finished with value: 0.8471542 and parameters: {'n_estimators': 990, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.6545868614325019, 'colsample_bytree': 0.5646284856916961, 'eta': 0.1480752912951422, 'learning_rate': 0.10045001652474819, 'reg_alpha': 1, 'reg_lambda': 4, 'gamma': 1}. Best is trial 1 with value: 0.8471542.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:53,341]\u001b[0m Trial 2 finished with value: 0.8365617000000001 and parameters: {'n_estimators': 260, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.7068986543004829, 'colsample_bytree': 0.3623947911759654, 'eta': 0.15981294259886059, 'learning_rate': 0.007164040994508561, 'reg_alpha': 3, 'reg_lambda': 5, 'gamma': 2}. Best is trial 1 with value: 0.8471542.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:53:59,239]\u001b[0m Trial 3 finished with value: 0.8482458999999999 and parameters: {'n_estimators': 397, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.35479535742410234, 'colsample_bytree': 0.8755479968655149, 'eta': 0.05036639427883523, 'learning_rate': 0.16260618058472626, 'reg_alpha': 2, 'reg_lambda': 1, 'gamma': 4}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:04,952]\u001b[0m Trial 4 finished with value: 0.8473219999999999 and parameters: {'n_estimators': 464, 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.21280292115283592, 'colsample_bytree': 0.8023649163397923, 'eta': 0.14513058622325953, 'learning_rate': 0.15451894218098053, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 5}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:05,605]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:11,260]\u001b[0m Trial 6 finished with value: 0.8429714 and parameters: {'n_estimators': 110, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.41365062658337487, 'colsample_bytree': 0.5670462065953317, 'eta': 0.19783410801409623, 'learning_rate': 0.2713921183800937, 'reg_alpha': 0, 'reg_lambda': 1, 'gamma': 4}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:11,412]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:11,595]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:11,697]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:11,828]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:12,098]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:12,327]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:12,442]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:12,563]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:12,748]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:13,034]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:13,219]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:13,354]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:13,477]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:13,689]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:15,284]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:16,119]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:16,239]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:16,453]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:17,191]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:17,362]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:21,215]\u001b[0m Trial 27 finished with value: 0.8432621999999999 and parameters: {'n_estimators': 524, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8115880132678919, 'colsample_bytree': 0.9007033398116813, 'eta': 0.13349137409359985, 'learning_rate': 0.27569846213802623, 'reg_alpha': 0, 'reg_lambda': 3, 'gamma': 0}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:21,336]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:21,455]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:21,597]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:21,767]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:25,559]\u001b[0m Trial 32 finished with value: 0.8477007000000001 and parameters: {'n_estimators': 278, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.9034618730224999, 'colsample_bytree': 0.8566613908833709, 'eta': 0.14489657285937588, 'learning_rate': 0.13164543977146648, 'reg_alpha': 0, 'reg_lambda': 4, 'gamma': 0}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:25,715]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,307]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,410]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,514]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,688]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,819]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:26,914]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:27,111]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:30,481]\u001b[0m Trial 41 finished with value: 0.8424598 and parameters: {'n_estimators': 965, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8882658417575469, 'colsample_bytree': 0.9160367558807012, 'eta': 0.12012421455078555, 'learning_rate': 0.3317636533736088, 'reg_alpha': 0, 'reg_lambda': 3, 'gamma': 0}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:34,117]\u001b[0m Trial 42 finished with value: 0.8453647 and parameters: {'n_estimators': 856, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.9427714834186506, 'colsample_bytree': 0.8075010329458164, 'eta': 0.1386539622997399, 'learning_rate': 0.21664960947515272, 'reg_alpha': 0, 'reg_lambda': 2, 'gamma': 0}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:34,240]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:38,766]\u001b[0m Trial 44 finished with value: 0.8473092000000001 and parameters: {'n_estimators': 947, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.932528007172025, 'colsample_bytree': 0.7930744137150368, 'eta': 0.10670200290431314, 'learning_rate': 0.16270824236249598, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 1}. Best is trial 3 with value: 0.8482458999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:39,946]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:40,052]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:40,178]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:40,278]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:40,386]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:43,357]\u001b[0m A new study created in memory with name: no-name-fee17d0c-1462-4284-92b3-e1c35efb0eac\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:47,230]\u001b[0m Trial 0 finished with value: 0.8435908 and parameters: {'n_estimators': 971, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.5266825248825842, 'colsample_bytree': 0.6529058678028928, 'eta': 0.1051674288146107, 'learning_rate': 0.009028817029721593, 'reg_alpha': 0, 'reg_lambda': 5, 'gamma': 2}. Best is trial 0 with value: 0.8435908.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:52,289]\u001b[0m Trial 1 finished with value: 0.8490074 and parameters: {'n_estimators': 449, 'max_depth': 12, 'min_child_weight': 6, 'subsample': 0.8198335880033842, 'colsample_bytree': 0.8164820224936744, 'eta': 0.17359142833780639, 'learning_rate': 0.10362071997320775, 'reg_alpha': 0, 'reg_lambda': 5, 'gamma': 4}. Best is trial 1 with value: 0.8490074.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:55,859]\u001b[0m Trial 2 finished with value: 0.8489456000000001 and parameters: {'n_estimators': 565, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.34984763682404363, 'colsample_bytree': 0.7550748006366572, 'eta': 0.2826868570255719, 'learning_rate': 0.06105806195043104, 'reg_alpha': 3, 'reg_lambda': 0, 'gamma': 0}. Best is trial 1 with value: 0.8490074.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:54:59,914]\u001b[0m Trial 3 finished with value: 0.8384085000000001 and parameters: {'n_estimators': 918, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.27990146459332915, 'colsample_bytree': 0.5612277931815999, 'eta': 0.15157276696208632, 'learning_rate': 0.012896350704189883, 'reg_alpha': 5, 'reg_lambda': 4, 'gamma': 4}. Best is trial 1 with value: 0.8490074.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:05,093]\u001b[0m Trial 4 finished with value: 0.8493359 and parameters: {'n_estimators': 820, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.32555275960325086, 'colsample_bytree': 0.3098986900592464, 'eta': 0.25522651709389493, 'learning_rate': 0.07778198221972718, 'reg_alpha': 4, 'reg_lambda': 2, 'gamma': 5}. Best is trial 4 with value: 0.8493359.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:05,303]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:05,531]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:05,625]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:06,877]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:07,202]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:07,315]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,038]\u001b[0m Trial 11 finished with value: 0.8495982 and parameters: {'n_estimators': 331, 'max_depth': 12, 'min_child_weight': 5, 'subsample': 0.8714198038445065, 'colsample_bytree': 0.9476742918204828, 'eta': 0.18419903866620524, 'learning_rate': 0.07176893015317687, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 4}. Best is trial 11 with value: 0.8495982.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,337]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,493]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,617]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,767]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:12,913]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:13,006]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:13,116]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:13,209]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:13,344]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:19,072]\u001b[0m Trial 21 finished with value: 0.8492882999999999 and parameters: {'n_estimators': 467, 'max_depth': 12, 'min_child_weight': 6, 'subsample': 0.8115588956642314, 'colsample_bytree': 0.852061185246106, 'eta': 0.18475140924276806, 'learning_rate': 0.0985851948773852, 'reg_alpha': 0, 'reg_lambda': 3, 'gamma': 4}. Best is trial 11 with value: 0.8495982.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:19,436]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:19,630]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:19,756]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:19,870]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:20,095]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:20,246]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:20,389]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:20,530]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:22,556]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 46.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:22,852]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:23,126]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:23,500]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:28,594]\u001b[0m Trial 34 finished with value: 0.8485951999999999 and parameters: {'n_estimators': 390, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.8527704425700805, 'colsample_bytree': 0.7183039029763669, 'eta': 0.14048949170414216, 'learning_rate': 0.06924620440052669, 'reg_alpha': 0, 'reg_lambda': 0, 'gamma': 3}. Best is trial 11 with value: 0.8495982.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:28,703]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:28,868]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:28,971]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,077]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,208]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,304]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,504]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,626]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,728]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,828]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:29,953]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:30,061]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:30,196]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:33,395]\u001b[0m Trial 48 finished with value: 0.8510088999999998 and parameters: {'n_estimators': 232, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.5666219673307161, 'colsample_bytree': 0.8302548774005686, 'eta': 0.26944288639678493, 'learning_rate': 0.054340195610661966, 'reg_alpha': 1, 'reg_lambda': 0, 'gamma': 3}. Best is trial 48 with value: 0.8510088999999998.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:33,496]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:35,094]\u001b[0m A new study created in memory with name: no-name-14a284a5-843d-480f-a638-4f6dbb16bfdb\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:41,957]\u001b[0m Trial 0 finished with value: 0.8512799 and parameters: {'n_estimators': 148, 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.8542252857065982, 'colsample_bytree': 0.5321186454109765, 'eta': 0.27458899050727675, 'learning_rate': 0.045908518330438026, 'reg_alpha': 2, 'reg_lambda': 0, 'gamma': 3}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:47,655]\u001b[0m Trial 1 finished with value: 0.8425272999999999 and parameters: {'n_estimators': 769, 'max_depth': 12, 'min_child_weight': 9, 'subsample': 0.6197815526281955, 'colsample_bytree': 0.4043361525217497, 'eta': 0.1205121798818314, 'learning_rate': 0.007225540606065654, 'reg_alpha': 3, 'reg_lambda': 0, 'gamma': 4}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:53,737]\u001b[0m Trial 2 finished with value: 0.8510422 and parameters: {'n_estimators': 332, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.9053331252993146, 'colsample_bytree': 0.9332957697970199, 'eta': 0.12440667132653839, 'learning_rate': 0.05274682378058555, 'reg_alpha': 3, 'reg_lambda': 5, 'gamma': 3}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:55:59,697]\u001b[0m Trial 3 finished with value: 0.8480551000000001 and parameters: {'n_estimators': 431, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.7366271682063439, 'colsample_bytree': 0.7690068172700406, 'eta': 0.1604889544930055, 'learning_rate': 0.029081258680258654, 'reg_alpha': 4, 'reg_lambda': 5, 'gamma': 3}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:04,061]\u001b[0m Trial 4 finished with value: 0.849043 and parameters: {'n_estimators': 448, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.2938230454597638, 'colsample_bytree': 0.4220084781525027, 'eta': 0.17544287335754638, 'learning_rate': 0.025185821945942596, 'reg_alpha': 0, 'reg_lambda': 3, 'gamma': 2}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:10,359]\u001b[0m Trial 5 finished with value: 0.8493646 and parameters: {'n_estimators': 90, 'max_depth': 11, 'min_child_weight': 3, 'subsample': 0.8202402213105535, 'colsample_bytree': 0.2989194761582865, 'eta': 0.06212070672208904, 'learning_rate': 0.06045197518419942, 'reg_alpha': 0, 'reg_lambda': 4, 'gamma': 2}. Best is trial 0 with value: 0.8512799.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:10,446]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:10,519]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:10,597]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:14,814]\u001b[0m Trial 9 finished with value: 0.8513065 and parameters: {'n_estimators': 913, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.971797729195671, 'colsample_bytree': 0.6992827568317901, 'eta': 0.07197068786818116, 'learning_rate': 0.08649934133149437, 'reg_alpha': 2, 'reg_lambda': 1, 'gamma': 3}. Best is trial 9 with value: 0.8513065.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:15,038]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:21,006]\u001b[0m Trial 11 finished with value: 0.8509604 and parameters: {'n_estimators': 714, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.9734040893062177, 'colsample_bytree': 0.5864733524897177, 'eta': 0.2940225725903584, 'learning_rate': 0.1957019075965911, 'reg_alpha': 2, 'reg_lambda': 0, 'gamma': 4}. Best is trial 9 with value: 0.8513065.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:25,203]\u001b[0m Trial 12 finished with value: 0.8488504000000001 and parameters: {'n_estimators': 211, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.47389903727052896, 'colsample_bytree': 0.5520116410413712, 'eta': 0.28912440488652125, 'learning_rate': 0.1477285989438961, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 1}. Best is trial 9 with value: 0.8513065.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:25,815]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:25,953]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:26,085]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:31,161]\u001b[0m Trial 16 finished with value: 0.8516482 and parameters: {'n_estimators': 860, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7224100036551211, 'colsample_bytree': 0.4847261553720111, 'eta': 0.19191500899933864, 'learning_rate': 0.10566194240455769, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 3}. Best is trial 16 with value: 0.8516482.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:35,470]\u001b[0m Trial 17 finished with value: 0.8513629999999999 and parameters: {'n_estimators': 839, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.520421807868084, 'colsample_bytree': 0.4041249873103299, 'eta': 0.1882107835103971, 'learning_rate': 0.10799259953885128, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 1}. Best is trial 16 with value: 0.8516482.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:35,690]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:35,822]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:35,996]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:40,491]\u001b[0m Trial 21 finished with value: 0.8524647 and parameters: {'n_estimators': 898, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.7022961491170201, 'colsample_bytree': 0.4330279462818676, 'eta': 0.20205665293260372, 'learning_rate': 0.08231962218389664, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 21 with value: 0.8524647.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:44,720]\u001b[0m Trial 22 finished with value: 0.8506664 and parameters: {'n_estimators': 595, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.6888593236625461, 'colsample_bytree': 0.47387796796916953, 'eta': 0.20333751765761823, 'learning_rate': 0.08886611312735446, 'reg_alpha': 0, 'reg_lambda': 2, 'gamma': 2}. Best is trial 21 with value: 0.8524647.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:44,876]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:46,528]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 33.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:46,651]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:52,749]\u001b[0m Trial 26 finished with value: 0.851271 and parameters: {'n_estimators': 634, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.6588639506973185, 'colsample_bytree': 0.4019194528642152, 'eta': 0.2186715471588606, 'learning_rate': 0.08632636606881955, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 2}. Best is trial 21 with value: 0.8524647.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:52,892]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:53,109]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:53,468]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:53,910]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:54,082]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:54,254]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:55,452]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:55,610]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:55,732]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:55,894]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:56,042]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:56,150]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:56,911]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:57,020]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:57,415]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:57,635]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:57,914]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:58,190]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:58,299]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:58,414]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:56:58,609]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:03,821]\u001b[0m Trial 48 finished with value: 0.8434449 and parameters: {'n_estimators': 493, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.623901728738732, 'colsample_bytree': 0.44348899348918414, 'eta': 0.05205760664740948, 'learning_rate': 0.23807816664477927, 'reg_alpha': 0, 'reg_lambda': 0, 'gamma': 2}. Best is trial 21 with value: 0.8524647.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:03,997]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:10,600]\u001b[0m A new study created in memory with name: no-name-810a2524-a27d-4b46-939f-1c91ec7cc833\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:16,616]\u001b[0m Trial 0 finished with value: 0.8503181 and parameters: {'n_estimators': 992, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.9132866479617399, 'colsample_bytree': 0.8586803197788553, 'eta': 0.14669446509032136, 'learning_rate': 0.056521008319313185, 'reg_alpha': 0, 'reg_lambda': 0, 'gamma': 5}. Best is trial 0 with value: 0.8503181.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:22,553]\u001b[0m Trial 1 finished with value: 0.8377547 and parameters: {'n_estimators': 163, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.6302417348786553, 'colsample_bytree': 0.2747562254957483, 'eta': 0.12940499137360628, 'learning_rate': 0.0058560204797180335, 'reg_alpha': 3, 'reg_lambda': 4, 'gamma': 2}. Best is trial 0 with value: 0.8503181.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:28,767]\u001b[0m Trial 2 finished with value: 0.8509949000000001 and parameters: {'n_estimators': 153, 'max_depth': 11, 'min_child_weight': 10, 'subsample': 0.873715926319121, 'colsample_bytree': 0.24312647863023998, 'eta': 0.11309489787927139, 'learning_rate': 0.102331037046585, 'reg_alpha': 3, 'reg_lambda': 1, 'gamma': 5}. Best is trial 2 with value: 0.8509949000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:34,282]\u001b[0m Trial 3 finished with value: 0.8516021 and parameters: {'n_estimators': 67, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.5253762727429121, 'colsample_bytree': 0.29955944639001225, 'eta': 0.16837528677662372, 'learning_rate': 0.05751967529351635, 'reg_alpha': 1, 'reg_lambda': 1, 'gamma': 5}. Best is trial 3 with value: 0.8516021.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:38,028]\u001b[0m Trial 4 finished with value: 0.8435615000000001 and parameters: {'n_estimators': 102, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.31557237674426475, 'colsample_bytree': 0.6460014504918152, 'eta': 0.2643330385360912, 'learning_rate': 0.02282274451492976, 'reg_alpha': 5, 'reg_lambda': 3, 'gamma': 3}. Best is trial 3 with value: 0.8516021.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:38,182]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:40,690]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 46.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:40,786]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:43,230]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 49.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:43,299]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:43,432]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:43,570]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:48,719]\u001b[0m Trial 12 finished with value: 0.8503097000000001 and parameters: {'n_estimators': 645, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.458770380634431, 'colsample_bytree': 0.5405635843100844, 'eta': 0.17832850994434485, 'learning_rate': 0.132339955426395, 'reg_alpha': 2, 'reg_lambda': 1, 'gamma': 4}. Best is trial 3 with value: 0.8516021.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:54,302]\u001b[0m Trial 13 finished with value: 0.8512638000000001 and parameters: {'n_estimators': 268, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.7093653058134507, 'colsample_bytree': 0.21573504641644833, 'eta': 0.1129658394807118, 'learning_rate': 0.12317185375547117, 'reg_alpha': 1, 'reg_lambda': 2, 'gamma': 5}. Best is trial 3 with value: 0.8516021.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:54,514]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:54,663]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:57,111]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:57,237]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:57,339]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:57:57,524]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:00,746]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 56.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:00,862]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:01,122]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:01,413]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:01,613]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:01,725]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:01,884]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:03,617]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:03,738]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:05,231]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 31.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:05,344]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:08,140]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 60.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:09,299]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:09,422]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:09,589]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:10,084]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:12,952]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 62.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:13,071]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:16,439]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 73.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:16,550]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:16,658]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:16,818]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:17,989]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,116]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,242]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,398]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,555]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,729]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:18,897]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:19,146]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:19,900]\u001b[0m A new study created in memory with name: no-name-ae880e11-d0f9-4960-9c5f-e06c2218adbd\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:23,929]\u001b[0m Trial 0 finished with value: 0.8480087 and parameters: {'n_estimators': 437, 'max_depth': 2, 'min_child_weight': 1, 'subsample': 0.8180584353843134, 'colsample_bytree': 0.3616853623554945, 'eta': 0.09353484086952309, 'learning_rate': 0.036797988642080955, 'reg_alpha': 4, 'reg_lambda': 4, 'gamma': 4}. Best is trial 0 with value: 0.8480087.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:27,928]\u001b[0m Trial 1 finished with value: 0.8498405 and parameters: {'n_estimators': 676, 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.36403320772585473, 'colsample_bytree': 0.8605142856486971, 'eta': 0.0897740542725756, 'learning_rate': 0.16322434508747627, 'reg_alpha': 4, 'reg_lambda': 4, 'gamma': 2}. Best is trial 1 with value: 0.8498405.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:31,875]\u001b[0m Trial 2 finished with value: 0.8388824999999999 and parameters: {'n_estimators': 66, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.4901424961328358, 'colsample_bytree': 0.2550099536880092, 'eta': 0.13978781332933965, 'learning_rate': 0.007803902546343822, 'reg_alpha': 2, 'reg_lambda': 3, 'gamma': 5}. Best is trial 1 with value: 0.8498405.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:35,052]\u001b[0m Trial 3 finished with value: 0.8463246 and parameters: {'n_estimators': 856, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.2278334201207578, 'colsample_bytree': 0.49961024901086254, 'eta': 0.2569117785674864, 'learning_rate': 0.4674074960160635, 'reg_alpha': 3, 'reg_lambda': 4, 'gamma': 4}. Best is trial 1 with value: 0.8498405.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:38,668]\u001b[0m Trial 4 finished with value: 0.8503288999999998 and parameters: {'n_estimators': 350, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.47559315004447805, 'colsample_bytree': 0.53673488418848, 'eta': 0.28911236823666525, 'learning_rate': 0.29191997841372636, 'reg_alpha': 3, 'reg_lambda': 1, 'gamma': 3}. Best is trial 4 with value: 0.8503288999999998.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:38,791]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:38,859]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:38,938]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:39,004]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:39,341]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:43,361]\u001b[0m Trial 10 finished with value: 0.8485235000000001 and parameters: {'n_estimators': 272, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.9070372190217124, 'colsample_bytree': 0.5447779216718658, 'eta': 0.29845271280169533, 'learning_rate': 0.1475463338149822, 'reg_alpha': 0, 'reg_lambda': 2, 'gamma': 2}. Best is trial 4 with value: 0.8503288999999998.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:43,475]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:43,584]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:43,697]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:44,048]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:44,371]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:44,482]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:48,063]\u001b[0m Trial 17 finished with value: 0.8457568 and parameters: {'n_estimators': 746, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.31856307432500963, 'colsample_bytree': 0.41953311516075475, 'eta': 0.29664513562832834, 'learning_rate': 0.2543783291232427, 'reg_alpha': 2, 'reg_lambda': 2, 'gamma': 0}. Best is trial 4 with value: 0.8503288999999998.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:48,207]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:48,323]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:48,721]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:49,076]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:50,598]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 30.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:50,981]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:51,164]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:52,496]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 28.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:52,879]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:57,372]\u001b[0m Trial 27 finished with value: 0.8502803999999999 and parameters: {'n_estimators': 631, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.8926168565303236, 'colsample_bytree': 0.26810857811242667, 'eta': 0.28240167595971916, 'learning_rate': 0.4983897749967133, 'reg_alpha': 4, 'reg_lambda': 3, 'gamma': 3}. Best is trial 4 with value: 0.8503288999999998.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:58:57,497]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:01,736]\u001b[0m Trial 29 finished with value: 0.8506755 and parameters: {'n_estimators': 815, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.7890210467576517, 'colsample_bytree': 0.3200033428589283, 'eta': 0.27744310552867185, 'learning_rate': 0.32121468799085134, 'reg_alpha': 5, 'reg_lambda': 3, 'gamma': 4}. Best is trial 29 with value: 0.8506755.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:05,965]\u001b[0m Trial 30 finished with value: 0.8515018 and parameters: {'n_estimators': 812, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.8330009792627534, 'colsample_bytree': 0.31211501311065004, 'eta': 0.2751927437564667, 'learning_rate': 0.3251563519752002, 'reg_alpha': 5, 'reg_lambda': 3, 'gamma': 4}. Best is trial 30 with value: 0.8515018.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:10,049]\u001b[0m Trial 31 finished with value: 0.8516149000000001 and parameters: {'n_estimators': 805, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.8351681935475188, 'colsample_bytree': 0.31957165439166124, 'eta': 0.2767323685953197, 'learning_rate': 0.3111700723827284, 'reg_alpha': 5, 'reg_lambda': 3, 'gamma': 4}. Best is trial 31 with value: 0.8516149000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:10,310]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:10,572]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:10,681]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:10,801]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:15,237]\u001b[0m Trial 36 finished with value: 0.8506514 and parameters: {'n_estimators': 897, 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9486430840428987, 'colsample_bytree': 0.2177824500513699, 'eta': 0.2548587046025701, 'learning_rate': 0.3691361670285032, 'reg_alpha': 5, 'reg_lambda': 3, 'gamma': 4}. Best is trial 31 with value: 0.8516149000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:15,467]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:15,772]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:16,099]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:16,205]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:16,503]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:16,906]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:17,015]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:17,125]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:17,235]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:17,341]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:21,861]\u001b[0m Trial 47 finished with value: 0.8508391 and parameters: {'n_estimators': 590, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.9631955760854622, 'colsample_bytree': 0.3697824859837142, 'eta': 0.23061401207215662, 'learning_rate': 0.28779871032834997, 'reg_alpha': 3, 'reg_lambda': 3, 'gamma': 3}. Best is trial 31 with value: 0.8516149000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:23,902]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 49.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:24,027]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:30,660]\u001b[0m A new study created in memory with name: no-name-920f4b57-c6d0-4b6d-b820-0a14f8f50390\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:33,906]\u001b[0m Trial 0 finished with value: 0.8472609999999999 and parameters: {'n_estimators': 219, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.2044829931743184, 'colsample_bytree': 0.37124435977542264, 'eta': 0.23578150691375044, 'learning_rate': 0.46952043722252784, 'reg_alpha': 4, 'reg_lambda': 3, 'gamma': 2}. Best is trial 0 with value: 0.8472609999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:37,025]\u001b[0m Trial 1 finished with value: 0.8514535999999999 and parameters: {'n_estimators': 655, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6275312190954656, 'colsample_bytree': 0.6149529725213627, 'eta': 0.15304600526811835, 'learning_rate': 0.0721301501263532, 'reg_alpha': 2, 'reg_lambda': 5, 'gamma': 3}. Best is trial 1 with value: 0.8514535999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:40,342]\u001b[0m Trial 2 finished with value: 0.8445002 and parameters: {'n_estimators': 476, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6910330200448512, 'colsample_bytree': 0.9192706908708572, 'eta': 0.19708830128990018, 'learning_rate': 0.2332594081410524, 'reg_alpha': 2, 'reg_lambda': 1, 'gamma': 0}. Best is trial 1 with value: 0.8514535999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:42,910]\u001b[0m Trial 3 finished with value: 0.8514841000000001 and parameters: {'n_estimators': 923, 'max_depth': 2, 'min_child_weight': 7, 'subsample': 0.7860621784449304, 'colsample_bytree': 0.48741081751274146, 'eta': 0.13674700717319255, 'learning_rate': 0.09910046864931306, 'reg_alpha': 3, 'reg_lambda': 1, 'gamma': 3}. Best is trial 3 with value: 0.8514841000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,144]\u001b[0m Trial 4 finished with value: 0.8517499000000001 and parameters: {'n_estimators': 671, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.9708535900175743, 'colsample_bytree': 0.3671257404668117, 'eta': 0.16109170659427258, 'learning_rate': 0.12021742622313972, 'reg_alpha': 3, 'reg_lambda': 2, 'gamma': 2}. Best is trial 4 with value: 0.8517499000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,414]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,477]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,539]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,610]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,690]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:46,998]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:47,156]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:47,537]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:47,724]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:47,865]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:48,754]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:48,996]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:49,809]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 20.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:49,977]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,160]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,304]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,432]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,570]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,688]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:50,924]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:51,087]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:51,220]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:51,343]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:51,484]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:51,624]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:52,610]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:52,721]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:52,841]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:52,973]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:53,083]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:53,189]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:56,904]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 77.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:57,092]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:57,218]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:57,338]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:57,459]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:57,594]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:58,366]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 13:59:58,479]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:02,616]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 95.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:02,732]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:02,851]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:03,206]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:03,343]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-10-26 14:00:03,461]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the XGBoost model, uses optuna for HP tuning, get accuracy, indices and probabilities for each fold\n",
    "xgb_two_k_fold_results, xgb_two_test_sets_index, xgb_two_predicted_probas, xgb_two_observed, xgb_two_shap, _, xgb_two_hps = ml_help.k_fold_accuracies(X_two, y, strat, False, 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175cc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in dictionary\n",
    "xgb_two_results = {'X':X_two, \n",
    "                'probas':xgb_two_predicted_probas, \n",
    "                'observed':xgb_two_observed, \n",
    "                'shap':xgb_two_shap\n",
    "                }\n",
    "\n",
    "with open(dict_directory+\"xgb_results_two\", \"wb\") as fp:   \n",
    "    #Pickling \n",
    "    pickle.dump(xgb_two_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f91cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mean accuracy to a table for easy perusal\n",
    "mean_results = ml_help.add_mean_to_df(mean_results, xgb_two_k_fold_results, 'xgb', 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2e11c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>mae</th>\n",
       "      <th>logloss</th>\n",
       "      <th>brier</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>predicted_positive_rate</th>\n",
       "      <th>observed_positive_rate</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>specificity</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.849698</td>\n",
       "      <td>0.127742</td>\n",
       "      <td>0.227546</td>\n",
       "      <td>0.063908</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.755355</td>\n",
       "      <td>0.249717</td>\n",
       "      <td>0.773999</td>\n",
       "      <td>0.375254</td>\n",
       "      <td>0.295988</td>\n",
       "      <td>0.093949</td>\n",
       "      <td>0.773999</td>\n",
       "      <td>0.246469</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.763765</td>\n",
       "      <td>xgb</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc       mae   logloss     brier  threshold  accuracy  precision  \\\n",
       "mean  0.849698  0.127742  0.227546  0.063908      0.084  0.755355   0.249717   \n",
       "\n",
       "        recall        f1  predicted_positive_rate  observed_positive_rate  \\\n",
       "mean  0.773999  0.375254                 0.295988                0.093949   \n",
       "\n",
       "           tpr       fpr  specificity  balanced_accuracy model features  \n",
       "mean  0.773999  0.246469     0.753531           0.763765   xgb      two  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df78c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a probability column to the whole dataset to ensure \n",
    "df_with_probas = ml_help.add_proba_col(df_with_probas, xgb_two_test_sets_index, xgb_two_predicted_probas , 'probas_xgb_two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb3a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-fold results\n",
    "xgb_two_k_fold_results.to_csv(k_fold_results_directory+'xgb_two.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd38e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters\n",
    "pd.DataFrame(xgb_two_hps).to_csv('../../results/hyperparameters/xgb_two.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9163b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe with all predicted probas\n",
    "df_with_probas.to_csv('../../results/probability_results/xgb_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da123f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cgm_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11957cbc5b69a14c5eed2137b8c383ab027096f0c9ca0d6fd5201f30e0447e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
