{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "471306f4-e3e1-4767-a930-1a5f4f891993",
   "metadata": {},
   "source": [
    "# 4. Heatmap (countour plot) of model\n",
    "This notebook focuses on visualizing the predictions of a machine learning model in the form of a heatmap, or contour plot. The objective is to create a heatmap that represents how the model's predicted probability changes across two variables: starting glucose levels and the duration of exercise. Such a visualization can provide valuable insights into how these two factors potentially influence the risk of an outcome, as predicted by the model.\n",
    "\n",
    "Outline:\n",
    "\n",
    "1. Load Packages and Data: In this section, we load the necessary packages and the trained models.\n",
    "2. Create Meshgrid: The meshgrid is a way to generate a 2D grid of coordinates, which we use for our x (starting glucose) and y (duration of exercise) values. This allows us to evaluate the model's predictions across a grid of combinations of starting glucose levels and exercise durations.\n",
    "3. Get Model Predictions: Here, we evaluate the trained models on our grid of data points, obtaining a prediction for each point.\n",
    "4. Reformat Data for Contour Plot: Before plotting, we need to organize the predictions in a way that's compatible with contour plotting.\n",
    "5. Plot Heatmap: Finally, we plot the heatmap (contour plot) using Matplotlib. We visualize the results both in mg/dL and mmol/L units for glucose levels, providing a comprehensive view of how the predicted probabilities vary across different starting glucose concentrations and exercise durations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe713158-caf0-44d1-bfc8-fa6067d4a5fb",
   "metadata": {},
   "source": [
    "## 4.0. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "582eea87-0f2c-43cf-b2c1-af2ed8bfd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, gridspec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ml_helper as ml_help\n",
    "import os\n",
    "from numpy import argmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd0699f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../data/tidy_data/final_df/'\n",
    "# Read data\n",
    "df = pd.read_csv(directory + 'ho_df.csv')\n",
    "strat = df['stratify'] \n",
    "X = pd.read_csv(directory + 'ho_X.csv')\n",
    "y = df['y'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9837f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(i):\n",
    "    with open(i, \"rb\") as fp:   \n",
    "        #Unpickling \n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2765d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../results/models/xgb/'\n",
    "models_two = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('two_'):\n",
    "        with open(directory + filename, 'rb') as file:\n",
    "            models_two.append(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f1402fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_all = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('all_'):\n",
    "        with open(directory + filename, 'rb') as file:\n",
    "            models_all.append(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0db300ff-9178-40c3-9ce5-1d3f0f6b098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "test_df = X[['start_glc', 'duration']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e7b0b6d-8734-4938-bca3-50752f776f63",
   "metadata": {},
   "source": [
    "## 4.2. Get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b96793f8-9d4a-40c7-bc1c-7ee638ca1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for predictions\n",
    "predictions = []\n",
    "for model in models_two:\n",
    "    y_probas = model.predict_proba(test_df)\n",
    "    y_probas = y_probas[:, 1].flatten()\n",
    "    predictions.append(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "32b4e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of the 10 model's predictions \n",
    "predicted_proba_two = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b133fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for predictions\n",
    "predictions_all = []\n",
    "for model in models_all:\n",
    "    y_probas = model.predict_proba(X)\n",
    "    y_probas = y_probas[:, 1].flatten()\n",
    "    predictions_all.append(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of the 10 model's predictions \n",
    "predicted_proba_all = np.mean(predictions_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8b5310ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852290650331952"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, predicted_proba_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7578c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = []\n",
    "\n",
    "# Create lists for k-fold results\n",
    "threshold_accuracy = []\n",
    "threshold_precision = []\n",
    "threshold_recall = []\n",
    "threshold_f1 = []\n",
    "threshold_predicted_positive_rate = []\n",
    "threshold_observed_positive_rate = []\n",
    "threshold_true_positive_rate = []\n",
    "threshold_false_positive_rate = []\n",
    "threshold_specificity = []\n",
    "threshold_balanced_accuracy = []\n",
    "\n",
    "# Set up thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "# Loop through increments in probability of survival\n",
    "for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "    # Get whether passengers survive using cutoff\n",
    "    predicted_survived = predicted_proba >= cutoff\n",
    "    # Call accuracy measures function\n",
    "    accuracy = ml_help.calculate_accuracy(y, predicted_survived)\n",
    "    # Add accuracy scores to lists\n",
    "    threshold_accuracy.append(accuracy['accuracy'])\n",
    "    threshold_precision.append(accuracy['precision'])\n",
    "    threshold_recall.append(accuracy['recall'])\n",
    "    threshold_f1.append(accuracy['f1'])\n",
    "    threshold_predicted_positive_rate.append(\n",
    "            accuracy['predicted_positive_rate'])\n",
    "    threshold_observed_positive_rate.append(\n",
    "            accuracy['observed_positive_rate'])\n",
    "    threshold_true_positive_rate.append(accuracy['true_positive_rate'])\n",
    "    threshold_false_positive_rate.append(accuracy['false_positive_rate'])\n",
    "    threshold_specificity.append(accuracy['specificity'])\n",
    "    threshold_balanced_accuracy.append((accuracy['specificity']+accuracy['recall'])/2)\n",
    "\n",
    "# Select threshold with the best balance between sensitivity and specificity\n",
    "ix = argmax(threshold_balanced_accuracy)\n",
    "roc_auc = roc_auc_score(y, predicted_proba)\n",
    "\n",
    "overall_results.append([roc_auc, thresholds[ix], threshold_accuracy[ix], threshold_precision[ix],\n",
    "                threshold_recall[ix], threshold_f1[ix], threshold_predicted_positive_rate[ix],\n",
    "                threshold_observed_positive_rate[ix], threshold_true_positive_rate[ix], \n",
    "                threshold_false_positive_rate[ix], threshold_specificity[ix], \n",
    "                threshold_balanced_accuracy[ix]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "35dafa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d27600ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, y):\n",
    "        overall_results = []\n",
    "        for probs in predictions: \n",
    "                # Create lists for k-fold results\n",
    "                threshold_accuracy = []\n",
    "                threshold_precision = []\n",
    "                threshold_recall = []\n",
    "                threshold_f1 = []\n",
    "                threshold_predicted_positive_rate = []\n",
    "                threshold_observed_positive_rate = []\n",
    "                threshold_true_positive_rate = []\n",
    "                threshold_false_positive_rate = []\n",
    "                threshold_specificity = []\n",
    "                threshold_balanced_accuracy = []\n",
    "\n",
    "                # Set up thresholds\n",
    "                thresholds = np.arange(0, 1.01, 0.01)\n",
    "                # Loop through increments in probability of survival\n",
    "                for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "                        # Get whether passengers survive using cutoff\n",
    "                        predicted_survived = probs >= cutoff\n",
    "                        # Call accuracy measures function\n",
    "                        accuracy = ml_help.calculate_accuracy(y, predicted_survived)\n",
    "                        # Add accuracy scores to lists\n",
    "                        threshold_accuracy.append(accuracy['accuracy'])\n",
    "                        threshold_precision.append(accuracy['precision'])\n",
    "                        threshold_recall.append(accuracy['recall'])\n",
    "                        threshold_f1.append(accuracy['f1'])\n",
    "                        threshold_predicted_positive_rate.append(\n",
    "                                accuracy['predicted_positive_rate'])\n",
    "                        threshold_observed_positive_rate.append(\n",
    "                                accuracy['observed_positive_rate'])\n",
    "                        threshold_true_positive_rate.append(accuracy['true_positive_rate'])\n",
    "                        threshold_false_positive_rate.append(accuracy['false_positive_rate'])\n",
    "                        threshold_specificity.append(accuracy['specificity'])\n",
    "                        threshold_balanced_accuracy.append((accuracy['specificity']+accuracy['recall'])/2)\n",
    "\n",
    "                # Select threshold with the best balance between sensitivity and specificity\n",
    "                ix = argmax(threshold_balanced_accuracy)\n",
    "                roc_auc = roc_auc_score(y, probs)\n",
    "\n",
    "                overall_results.append([roc_auc, thresholds[ix], threshold_accuracy[ix], threshold_precision[ix],\n",
    "                                threshold_recall[ix], threshold_f1[ix], threshold_predicted_positive_rate[ix],\n",
    "                                threshold_observed_positive_rate[ix], threshold_true_positive_rate[ix], \n",
    "                                threshold_false_positive_rate[ix], threshold_specificity[ix], \n",
    "                                threshold_balanced_accuracy[ix]])\n",
    "        overall_results = pd.DataFrame(overall_results)\n",
    "        overall_results.columns=['roc_auc', 'threshold','accuracy', 'precision', 'recall', 'f1','predicted_positive_rate', 'observed_positive_rate', 'tpr','fpr','specificity','balanced_accuracy']\n",
    "        return overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "679c55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_simple = calculate_metrics(predictions, y)\n",
    "metrics_all = calculate_metrics(predictions_all, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5ea60020",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_results_xgb = ml_help.calculate_confidence_intervals(metrics_simple, ['roc_auc','balanced_accuracy', 'recall', 'specificity']) #lr_two_k_fold_results\n",
    "all_results_xgb = ml_help.calculate_confidence_intervals(metrics_all, ['roc_auc','balanced_accuracy', 'recall', 'specificity']) #lr_two_k_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1434aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_xgb['two'] = simple_results_xgb['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "82308a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>recall</td>\n",
       "      <td>specificity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>0.88 (0.87, 0.88)</td>\n",
       "      <td>0.79 (0.79, 0.80)</td>\n",
       "      <td>0.79 (0.77, 0.82)</td>\n",
       "      <td>0.79 (0.76, 0.82)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>0.86 (0.86, 0.86)</td>\n",
       "      <td>0.77 (0.77, 0.77)</td>\n",
       "      <td>0.74 (0.72, 0.76)</td>\n",
       "      <td>0.79 (0.77, 0.82)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                  1                  2  \\\n",
       "metric            roc_auc  balanced_accuracy             recall   \n",
       "value   0.88 (0.87, 0.88)  0.79 (0.79, 0.80)  0.79 (0.77, 0.82)   \n",
       "two     0.86 (0.86, 0.86)  0.77 (0.77, 0.77)  0.74 (0.72, 0.76)   \n",
       "\n",
       "                        3  \n",
       "metric        specificity  \n",
       "value   0.79 (0.76, 0.82)  \n",
       "two     0.79 (0.77, 0.82)  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_xgb.T#.pivot_table(columns='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48c4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cgm_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11957cbc5b69a14c5eed2137b8c383ab027096f0c9ca0d6fd5201f30e0447e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
